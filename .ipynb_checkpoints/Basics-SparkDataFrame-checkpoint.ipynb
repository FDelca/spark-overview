{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad473979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fccb89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:27:34.505784Z",
     "start_time": "2022-05-25T17:27:34.502298Z"
    }
   },
   "outputs": [],
   "source": [
    "## Inside docker we do not need to use findspark\n",
    "# import findspark\n",
    "# findspark.init('/home/fdelca/Documents/spark/spark-2.1.2-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f5c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\r\n",
      "      ____              __\r\n",
      "     / __/__  ___ _____/ /__\r\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\r\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.5\r\n",
      "      /_/\r\n",
      "                        \r\n",
      "Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_252\r\n",
      "Branch HEAD\r\n",
      "Compiled by user centos on 2020-02-02T19:38:06Z\r\n",
      "Revision cee4ecbb16917fa85f02c635925e2687400aa56b\r\n",
      "Url https://gitbox.apache.org/repos/asf/spark.git\r\n",
      "Type --help for more information.\r\n"
     ]
    }
   ],
   "source": [
    "! pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db2b3a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.6\r\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173c12e",
   "metadata": {},
   "source": [
    "### Start a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ff179b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:23:18.087640Z",
     "start_time": "2022-05-25T17:23:18.083869Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a06050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:23:47.510655Z",
     "start_time": "2022-05-25T17:23:45.648117Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 10:50:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca0b13",
   "metadata": {},
   "source": [
    "## Spark DataFrames - Lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f9d90",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ff1f4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:27:49.329544Z",
     "start_time": "2022-05-25T17:27:47.923712Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(os.path.join('data','people.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "290afc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:28:07.388881Z",
     "start_time": "2022-05-25T17:28:06.972646Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f47049",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:28:38.093639Z",
     "start_time": "2022-05-25T17:28:38.087022Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Know the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043a025",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Depending on your datasource, spark can dtype everything as a string - be caution with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2411ba71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:30:55.317243Z",
     "start_time": "2022-05-25T17:30:55.312356Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a616370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:31:21.783353Z",
     "start_time": "2022-05-25T17:31:21.589025Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------+\n",
      "|summary|               age|   name|\n",
      "+-------+------------------+-------+\n",
      "|  count|                 2|      3|\n",
      "|   mean|              24.5|   null|\n",
      "| stddev|7.7781745930520225|   null|\n",
      "|    min|                19|   Andy|\n",
      "|    max|                30|Michael|\n",
      "+-------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary of your columns\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373c2f4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define Schema\n",
    "- Sometimes we need to clarify the datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8921ff20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:33:53.903274Z",
     "start_time": "2022-05-25T17:33:53.899116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bce1fc85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:35:34.114678Z",
     "start_time": "2022-05-25T17:35:34.109687Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_schema = [\n",
    "    StructField('age', IntegerType(), True), #Column name, Class instance (integer type), T/F can it be null?\n",
    "    StructField('name', StringType(), True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09fe7e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:35:54.517082Z",
     "start_time": "2022-05-25T17:35:54.513496Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b2b2ef5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:36:42.864236Z",
     "start_time": "2022-05-25T17:36:42.839861Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(os.path.join('data','people.json'), schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04651e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:36:54.655946Z",
     "start_time": "2022-05-25T17:36:54.650500Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6218a8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a3c4d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc6e975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:39:21.466721Z",
     "start_time": "2022-05-25T17:39:21.460409Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a column object\n",
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11c1aced",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:40:29.748598Z",
     "start_time": "2022-05-25T17:40:29.736781Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With select we can have a dataframe, and then we can add show() to see the column\n",
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11e4ef6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:40:36.454476Z",
     "start_time": "2022-05-25T17:40:36.405433Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "748e57a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:41:54.503130Z",
     "start_time": "2022-05-25T17:41:54.464834Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first two rows\n",
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcdbe232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:42:13.251409Z",
     "start_time": "2022-05-25T17:42:13.191438Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age', 'name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53990744",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create a dataframe with a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b0d0eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:43:29.587374Z",
     "start_time": "2022-05-25T17:43:29.542261Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+----------+\n",
      "| age|   name|double_age|\n",
      "+----+-------+----------+\n",
      "|null|Michael|      null|\n",
      "|  30|   Andy|        60|\n",
      "|  19| Justin|        38|\n",
      "+----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('double_age', df['age']*2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56fa4a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:43:53.349314Z",
     "start_time": "2022-05-25T17:43:53.306517Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4776f5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need to save it to a new variable, it is not an inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170851e3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad6cb56c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:44:51.977825Z",
     "start_time": "2022-05-25T17:44:51.930001Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|my_new_age|   name|\n",
      "+----------+-------+\n",
      "|      null|Michael|\n",
      "|        30|   Andy|\n",
      "|        19| Justin|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename a column\n",
    "df.withColumnRenamed('age', 'my_new_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc18b68",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using Pure SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bf0a4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Register the df as a temporary view in SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26bb4597",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:46:33.575374Z",
     "start_time": "2022-05-25T17:46:33.462095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86e62b54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:47:11.312107Z",
     "start_time": "2022-05-25T17:47:11.302134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results = spark.sql(\"SELECT * FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18e00bc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:47:17.874995Z",
     "start_time": "2022-05-25T17:47:17.831669Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02c25a37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:47:41.909947Z",
     "start_time": "2022-05-25T17:47:41.876711Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_results = spark.sql(\"SELECT * FROM people WHERE age=30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d002955b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T17:47:47.069792Z",
     "start_time": "2022-05-25T17:47:46.996202Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f64720",
   "metadata": {},
   "source": [
    "## Spark DataFrames - Lesson 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "141d4deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(os.path.join('data', 'appl_stock.csv'), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "477d2164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e324fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1de019",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fd9951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|              Open|             Close|\n",
      "+------------------+------------------+\n",
      "|        213.429998|        214.009998|\n",
      "|        214.599998|        214.379993|\n",
      "|        214.379993|        210.969995|\n",
      "|            211.75|            210.58|\n",
      "|        210.299994|211.98000499999998|\n",
      "|212.79999700000002|210.11000299999998|\n",
      "|209.18999499999998|        207.720001|\n",
      "|        207.870005|        210.650002|\n",
      "|210.11000299999998|            209.43|\n",
      "|210.92999500000002|            205.93|\n",
      "|        208.330002|        215.039995|\n",
      "|        214.910006|            211.73|\n",
      "|        212.079994|        208.069996|\n",
      "|206.78000600000001|            197.75|\n",
      "|202.51000200000001|        203.070002|\n",
      "|205.95000100000001|        205.940001|\n",
      "|        206.849995|        207.880005|\n",
      "|        204.930004|        199.289995|\n",
      "|        201.079996|        192.060003|\n",
      "|192.36999699999998|        194.729998|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One can use filter and select together, to make two different operations\n",
    "df.filter(\"Close < 500\").select([\"Open\", \"Close\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58e7b460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   Volume|\n",
      "+---------+\n",
      "|123432400|\n",
      "|150476200|\n",
      "|138040000|\n",
      "|119282800|\n",
      "|111902700|\n",
      "|115557400|\n",
      "|148614900|\n",
      "|151473000|\n",
      "|108223500|\n",
      "|148516900|\n",
      "|182501900|\n",
      "|153038200|\n",
      "|152038600|\n",
      "|220441900|\n",
      "|266424900|\n",
      "|466777500|\n",
      "|430642100|\n",
      "|293375600|\n",
      "|311488100|\n",
      "|187469100|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close'] < 500).select('Volume').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d448a9d",
   "metadata": {},
   "source": [
    "### Filter Data - Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "586d216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter method works similar to python\n",
    "df.filter((df['Close'] < 200) & (df['Open']>200)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9907caa",
   "metadata": {},
   "source": [
    "#### Use `collect()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f8911",
   "metadata": {},
   "source": [
    "When using `collect()` we got a list with the values of the filter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b05924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|               Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show()\n",
    "df.filter(df['Low'] == 197.16).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d511c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect() - we can work with this variable\n",
    "result = df.filter(df['Low'] == 197.16).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4e2b597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = result[0]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2de0dc78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2010, 1, 22, 0, 0),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "220441900"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can transform it into a dictionary\n",
    "display(row.asDict())\n",
    "# And select the value of interest\n",
    "row.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf735276",
   "metadata": {},
   "source": [
    "## GroupBy and Aggregate - Lesson 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5957809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(os.path.join('data','sales_info.csv'), inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c832ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a01f9d",
   "metadata": {},
   "source": [
    "### GroupBy a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1624958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Company|count|\n",
      "+-------+-----+\n",
      "|   APPL|    4|\n",
      "|   GOOG|    3|\n",
      "|     FB|    2|\n",
      "|   MSFT|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean(), max(), min(), sum(), count() [how many rows] - can be used in this method\n",
    "df.groupBy('Company').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b43d58",
   "metadata": {},
   "source": [
    "### Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3f42dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(Sales)|\n",
      "+----------+\n",
      "|     870.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Similar to python function\n",
    "df.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de990c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "efb0b4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 226:================================================>    (184 + 8) / 200]\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+-----------------+------------------+-------+\n",
      "|Company|Min Sales|Max Sales|        Avg Sales|        Stdv Sales|Sellers|\n",
      "+-------+---------+---------+-----------------+------------------+-------+\n",
      "|   APPL|    130.0|    750.0|            370.0|  268.824602048746|      4|\n",
      "|   GOOG|    120.0|    340.0|            220.0|111.35528725660043|      3|\n",
      "|     FB|    350.0|    870.0|            610.0| 367.6955262170047|      2|\n",
      "|   MSFT|    124.0|    600.0|322.3333333333333| 247.7182539364698|      3|\n",
      "+-------+---------+---------+-----------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data = df.groupby('Company')\n",
    "\n",
    "group_data.agg(functions.min('Sales').alias('Min Sales'), \n",
    "               functions.max('Sales').alias('Max Sales'), \n",
    "               functions.avg('Sales').alias('Avg Sales'),\n",
    "               functions.stddev('Sales').alias('Stdv Sales'),\n",
    "               functions.countDistinct('Person').alias('Sellers'),\n",
    "              ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5866e47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Average Sales|\n",
      "+-------------+\n",
      "|           11|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# With select one can apply to only one colummn\n",
    "\n",
    "df.select(functions.countDistinct('Sales').alias('Average Sales')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e9893",
   "metadata": {},
   "source": [
    "- **alias()** method allow us to give a name to the calculated column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3236198",
   "metadata": {},
   "source": [
    "#### Format Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d68a84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "382b52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_std = df.select(functions.stddev('Sales').alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e068fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_std.select(functions.format_number('std', 2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff83b731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   std|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Shortest way to get the same result\n",
    "df.select(\n",
    "    functions.stddev('Sales').alias('std')\n",
    ").select(\n",
    "    functions.format_number('std', 2).alias('std')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66479fa",
   "metadata": {},
   "source": [
    "### Order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "575e0ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ascending\n",
    "df.orderBy('Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "66c73f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descending - more work - maybe in spark 3 is already solved\n",
    "df.orderBy(df['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c7a02",
   "metadata": {},
   "source": [
    "## Missing Data - Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dfce97a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(os.path.join('data', 'ContainsNull.csv'), header=True, inferSchema=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3f412ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a491de",
   "metadata": {},
   "source": [
    "The function that deals with missing data is called `na` and it has different methods:\n",
    "- `drop()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed48c9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh : allows you to drop only rows that respect a certain argument\n",
    "    # In the example below, the row needs to have at least 2 null values to be dropped\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7d63f01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how : 'all', 'any'\n",
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdddeb",
   "metadata": {},
   "source": [
    "- `fill()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83a651bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2122563",
   "metadata": {},
   "source": [
    "**Spark** is intelligence enough to fill only the columns that have the same type as the value the user is asking to fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8a9ecce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+\n",
      "|  Id|     Name|Sales|\n",
      "+----+---------+-----+\n",
      "|emp1|     John| null|\n",
      "|emp2|FILLVALUE| null|\n",
      "|emp3|FILLVALUE|345.0|\n",
      "|emp4|    Cindy|456.0|\n",
      "+----+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('FILLVALUE').show() # There was no need to specify the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "839060c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show() # There was no need to specify the column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf43548",
   "metadata": {},
   "source": [
    "But the user can always specify the column that wants to be filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2d321dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('No Name', subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f34040f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "384da562",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = df.select(functions.mean(df['Sales'])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ecadcdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean_sales = mean_val[0].asDict()['avg(Sales)']\n",
    "mean_sales = mean_val[0][0]\n",
    "mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c1e4ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_sales, ['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c5ee10d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\n",
    "    df.select(functions.mean(df['Sales'])).collect()[0][0],\n",
    "    ['Sales']\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c197a4",
   "metadata": {},
   "source": [
    "## Dates and TimeStamps - Lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b581156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(os.path.join('data', 'appl_stock.csv'), inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9164b3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               Date|              Open|\n",
      "+-------------------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|\n",
      "|2010-01-05 00:00:00|        214.599998|\n",
      "|2010-01-06 00:00:00|        214.379993|\n",
      "|2010-01-07 00:00:00|            211.75|\n",
      "|2010-01-08 00:00:00|        210.299994|\n",
      "|2010-01-11 00:00:00|212.79999700000002|\n",
      "|2010-01-12 00:00:00|209.18999499999998|\n",
      "|2010-01-13 00:00:00|        207.870005|\n",
      "|2010-01-14 00:00:00|210.11000299999998|\n",
      "|2010-01-15 00:00:00|210.92999500000002|\n",
      "|2010-01-19 00:00:00|        208.330002|\n",
      "|2010-01-20 00:00:00|        214.910006|\n",
      "|2010-01-21 00:00:00|        212.079994|\n",
      "|2010-01-22 00:00:00|206.78000600000001|\n",
      "|2010-01-25 00:00:00|202.51000200000001|\n",
      "|2010-01-26 00:00:00|205.95000100000001|\n",
      "|2010-01-27 00:00:00|        206.849995|\n",
      "|2010-01-28 00:00:00|        204.930004|\n",
      "|2010-01-29 00:00:00|        201.079996|\n",
      "|2010-02-01 00:00:00|192.36999699999998|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['Date', 'Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6a8e7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month,\n",
    "                                   year, weekofyear, format_number, date_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aba505",
   "metadata": {},
   "source": [
    "**Hour/Month/Dayofmonth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5213da24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|month(Date)|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          1|\n",
      "|          2|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select(dayofmonth(df['Date'])).show()\n",
    "# df.select(hour(df['Date'])).show()\n",
    "df.select(month(df['Date'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bff5596",
   "metadata": {},
   "source": [
    "#### Average closing price per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9626d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = df.withColumn(colName='Year', col=year(df['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "abec24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039, Year=2010)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f95ace32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = newdf.groupBy('Year').agg(functions.avg('Close').alias('Avg Closing Price'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3bf15acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|Avg Closing Price|\n",
      "+----+-----------------+\n",
      "|2015|           120.04|\n",
      "|2013|           472.63|\n",
      "|2014|           295.40|\n",
      "|2012|           576.05|\n",
      "|2016|           104.60|\n",
      "|2010|           259.84|\n",
      "|2011|           364.00|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(['Year', format_number('Avg Closing Price', 2).alias('Avg Closing Price')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa02a99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
